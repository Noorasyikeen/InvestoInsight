{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892856ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68d8e371",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Input, Concatenate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b1453e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sorted_feature_matrix.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5b0f487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Date</th>\n",
       "      <th>Dividend</th>\n",
       "      <th>Volume</th>\n",
       "      <th>stock_price</th>\n",
       "      <th>fed_funds_rate</th>\n",
       "      <th>GDP</th>\n",
       "      <th>Tickers</th>\n",
       "      <th>debt_to_equity</th>\n",
       "      <th>EPS</th>\n",
       "      <th>return_on_equity</th>\n",
       "      <th>quick ratio</th>\n",
       "      <th>operating_ratio</th>\n",
       "      <th>inventory_turnover</th>\n",
       "      <th>pos_ma</th>\n",
       "      <th>neu_ma</th>\n",
       "      <th>neg_ma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69</td>\n",
       "      <td>2018-01-31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.315047e+06</td>\n",
       "      <td>51.919825</td>\n",
       "      <td>1.41</td>\n",
       "      <td>4500.182000</td>\n",
       "      <td>AAL</td>\n",
       "      <td>-134.253165</td>\n",
       "      <td>0.807713</td>\n",
       "      <td>-0.941772</td>\n",
       "      <td>0.484585</td>\n",
       "      <td>0.476713</td>\n",
       "      <td>1.930838</td>\n",
       "      <td>0.285655</td>\n",
       "      <td>0.519859</td>\n",
       "      <td>0.194486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>70</td>\n",
       "      <td>2018-02-28</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5.315047e+06</td>\n",
       "      <td>51.919825</td>\n",
       "      <td>1.42</td>\n",
       "      <td>4549.064667</td>\n",
       "      <td>AAL</td>\n",
       "      <td>-134.253165</td>\n",
       "      <td>0.807713</td>\n",
       "      <td>-0.941772</td>\n",
       "      <td>0.484585</td>\n",
       "      <td>0.476713</td>\n",
       "      <td>1.930838</td>\n",
       "      <td>0.320593</td>\n",
       "      <td>0.266028</td>\n",
       "      <td>0.413378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>71</td>\n",
       "      <td>2018-03-31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.558910e+06</td>\n",
       "      <td>49.781805</td>\n",
       "      <td>1.51</td>\n",
       "      <td>4597.947333</td>\n",
       "      <td>AAL</td>\n",
       "      <td>-134.253165</td>\n",
       "      <td>0.807713</td>\n",
       "      <td>-0.941772</td>\n",
       "      <td>0.484585</td>\n",
       "      <td>0.476713</td>\n",
       "      <td>1.930838</td>\n",
       "      <td>0.800789</td>\n",
       "      <td>0.142719</td>\n",
       "      <td>0.056492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1371</th>\n",
       "      <td>72</td>\n",
       "      <td>2018-04-30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.802773e+06</td>\n",
       "      <td>47.643785</td>\n",
       "      <td>1.69</td>\n",
       "      <td>4646.830000</td>\n",
       "      <td>AAL</td>\n",
       "      <td>-134.253165</td>\n",
       "      <td>0.807713</td>\n",
       "      <td>-0.941772</td>\n",
       "      <td>0.484585</td>\n",
       "      <td>0.476713</td>\n",
       "      <td>1.930838</td>\n",
       "      <td>0.563970</td>\n",
       "      <td>0.386575</td>\n",
       "      <td>0.049455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1837</th>\n",
       "      <td>73</td>\n",
       "      <td>2018-05-31</td>\n",
       "      <td>0.1</td>\n",
       "      <td>6.371183e+06</td>\n",
       "      <td>42.626889</td>\n",
       "      <td>1.70</td>\n",
       "      <td>4661.188333</td>\n",
       "      <td>AAL</td>\n",
       "      <td>-134.253165</td>\n",
       "      <td>0.807713</td>\n",
       "      <td>-0.941772</td>\n",
       "      <td>0.484585</td>\n",
       "      <td>0.476713</td>\n",
       "      <td>1.930838</td>\n",
       "      <td>0.029625</td>\n",
       "      <td>0.874360</td>\n",
       "      <td>0.096015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30519</th>\n",
       "      <td>133</td>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.599978e+07</td>\n",
       "      <td>13.983333</td>\n",
       "      <td>5.06</td>\n",
       "      <td>5053.623333</td>\n",
       "      <td>AAL</td>\n",
       "      <td>-14.844027</td>\n",
       "      <td>1.370834</td>\n",
       "      <td>-0.184719</td>\n",
       "      <td>0.644943</td>\n",
       "      <td>0.496469</td>\n",
       "      <td>1.729324</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.999418</td>\n",
       "      <td>0.000440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31001</th>\n",
       "      <td>134</td>\n",
       "      <td>2023-06-30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.599978e+07</td>\n",
       "      <td>13.983333</td>\n",
       "      <td>5.08</td>\n",
       "      <td>5053.623333</td>\n",
       "      <td>AAL</td>\n",
       "      <td>-16.338655</td>\n",
       "      <td>2.048105</td>\n",
       "      <td>-0.305131</td>\n",
       "      <td>0.655535</td>\n",
       "      <td>0.466129</td>\n",
       "      <td>1.679386</td>\n",
       "      <td>0.351945</td>\n",
       "      <td>0.197356</td>\n",
       "      <td>0.450699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31483</th>\n",
       "      <td>135</td>\n",
       "      <td>2023-07-31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.599978e+07</td>\n",
       "      <td>13.983333</td>\n",
       "      <td>5.08</td>\n",
       "      <td>5053.623333</td>\n",
       "      <td>AAL</td>\n",
       "      <td>-16.338655</td>\n",
       "      <td>2.048105</td>\n",
       "      <td>-0.305131</td>\n",
       "      <td>0.655535</td>\n",
       "      <td>0.466129</td>\n",
       "      <td>1.679386</td>\n",
       "      <td>0.444775</td>\n",
       "      <td>0.553510</td>\n",
       "      <td>0.001715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31964</th>\n",
       "      <td>136</td>\n",
       "      <td>2023-08-31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.599978e+07</td>\n",
       "      <td>13.983333</td>\n",
       "      <td>5.08</td>\n",
       "      <td>5053.623333</td>\n",
       "      <td>AAL</td>\n",
       "      <td>-16.338655</td>\n",
       "      <td>2.048105</td>\n",
       "      <td>-0.305131</td>\n",
       "      <td>0.655535</td>\n",
       "      <td>0.466129</td>\n",
       "      <td>1.679386</td>\n",
       "      <td>0.239710</td>\n",
       "      <td>0.502044</td>\n",
       "      <td>0.258246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32444</th>\n",
       "      <td>137</td>\n",
       "      <td>2023-09-30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.599978e+07</td>\n",
       "      <td>13.983333</td>\n",
       "      <td>5.08</td>\n",
       "      <td>5053.623333</td>\n",
       "      <td>AAL</td>\n",
       "      <td>-16.338655</td>\n",
       "      <td>2.048105</td>\n",
       "      <td>-0.305131</td>\n",
       "      <td>0.655535</td>\n",
       "      <td>0.466129</td>\n",
       "      <td>1.679386</td>\n",
       "      <td>0.888032</td>\n",
       "      <td>0.104769</td>\n",
       "      <td>0.007198</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0        Date  Dividend        Volume  stock_price  \\\n",
       "0              69  2018-01-31       0.0  5.315047e+06    51.919825   \n",
       "447            70  2018-02-28       0.1  5.315047e+06    51.919825   \n",
       "905            71  2018-03-31       0.0  5.558910e+06    49.781805   \n",
       "1371           72  2018-04-30       0.0  5.802773e+06    47.643785   \n",
       "1837           73  2018-05-31       0.1  6.371183e+06    42.626889   \n",
       "...           ...         ...       ...           ...          ...   \n",
       "30519         133  2023-05-31       0.0  2.599978e+07    13.983333   \n",
       "31001         134  2023-06-30       0.0  2.599978e+07    13.983333   \n",
       "31483         135  2023-07-31       0.0  2.599978e+07    13.983333   \n",
       "31964         136  2023-08-31       0.0  2.599978e+07    13.983333   \n",
       "32444         137  2023-09-30       0.0  2.599978e+07    13.983333   \n",
       "\n",
       "       fed_funds_rate          GDP Tickers  debt_to_equity       EPS  \\\n",
       "0                1.41  4500.182000     AAL     -134.253165  0.807713   \n",
       "447              1.42  4549.064667     AAL     -134.253165  0.807713   \n",
       "905              1.51  4597.947333     AAL     -134.253165  0.807713   \n",
       "1371             1.69  4646.830000     AAL     -134.253165  0.807713   \n",
       "1837             1.70  4661.188333     AAL     -134.253165  0.807713   \n",
       "...               ...          ...     ...             ...       ...   \n",
       "30519            5.06  5053.623333     AAL      -14.844027  1.370834   \n",
       "31001            5.08  5053.623333     AAL      -16.338655  2.048105   \n",
       "31483            5.08  5053.623333     AAL      -16.338655  2.048105   \n",
       "31964            5.08  5053.623333     AAL      -16.338655  2.048105   \n",
       "32444            5.08  5053.623333     AAL      -16.338655  2.048105   \n",
       "\n",
       "       return_on_equity  quick ratio  operating_ratio  inventory_turnover  \\\n",
       "0             -0.941772     0.484585         0.476713            1.930838   \n",
       "447           -0.941772     0.484585         0.476713            1.930838   \n",
       "905           -0.941772     0.484585         0.476713            1.930838   \n",
       "1371          -0.941772     0.484585         0.476713            1.930838   \n",
       "1837          -0.941772     0.484585         0.476713            1.930838   \n",
       "...                 ...          ...              ...                 ...   \n",
       "30519         -0.184719     0.644943         0.496469            1.729324   \n",
       "31001         -0.305131     0.655535         0.466129            1.679386   \n",
       "31483         -0.305131     0.655535         0.466129            1.679386   \n",
       "31964         -0.305131     0.655535         0.466129            1.679386   \n",
       "32444         -0.305131     0.655535         0.466129            1.679386   \n",
       "\n",
       "         pos_ma    neu_ma    neg_ma  \n",
       "0      0.285655  0.519859  0.194486  \n",
       "447    0.320593  0.266028  0.413378  \n",
       "905    0.800789  0.142719  0.056492  \n",
       "1371   0.563970  0.386575  0.049455  \n",
       "1837   0.029625  0.874360  0.096015  \n",
       "...         ...       ...       ...  \n",
       "30519  0.000142  0.999418  0.000440  \n",
       "31001  0.351945  0.197356  0.450699  \n",
       "31483  0.444775  0.553510  0.001715  \n",
       "31964  0.239710  0.502044  0.258246  \n",
       "32444  0.888032  0.104769  0.007198  \n",
       "\n",
       "[69 rows x 17 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['Tickers']==\"AAL\"]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a6055fb5",
   "metadata": {},
   "source": [
    "df = df[['Unnamed: 0', 'date', 'symbol', 'Dividend', 'volume', 'fed_funds_rate',\n",
    "#        'gdp', 'debt_to_equity', 'eps', 'return_on_equity', 'quick_ratio',\n",
    "       'operating_ratio', 'inventory_turnover', 'sa_neu','sa_pos', 'sa_neg', 'stock_price']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51cba123",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32914 entries, 0 to 32913\n",
      "Data columns (total 17 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   Unnamed: 0          32914 non-null  int64  \n",
      " 1   Date                32914 non-null  object \n",
      " 2   Dividend            32914 non-null  float64\n",
      " 3   Volume              32914 non-null  float64\n",
      " 4   stock_price         32914 non-null  float64\n",
      " 5   fed_funds_rate      32914 non-null  float64\n",
      " 6   GDP                 32914 non-null  float64\n",
      " 7   Tickers             32914 non-null  object \n",
      " 8   debt_to_equity      32914 non-null  float64\n",
      " 9   EPS                 32914 non-null  float64\n",
      " 10  return_on_equity    32914 non-null  float64\n",
      " 11  quick ratio         32914 non-null  float64\n",
      " 12  operating_ratio     32914 non-null  float64\n",
      " 13  inventory_turnover  32914 non-null  float64\n",
      " 14  pos_ma              32914 non-null  float64\n",
      " 15  neu_ma              32914 non-null  float64\n",
      " 16  neg_ma              32914 non-null  float64\n",
      "dtypes: float64(14), int64(1), object(2)\n",
      "memory usage: 4.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09633157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data.drop(columns=['Unnamed: 0']).set_index(['date', 'symbol', 'stock_price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5c446e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 100,\n",
       " 101,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 105,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " 109,\n",
       " 111,\n",
       " 113,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 119,\n",
       " 120,\n",
       " 122,\n",
       " 123,\n",
       " 124,\n",
       " 126,\n",
       " 128,\n",
       " 130,\n",
       " 131,\n",
       " 132,\n",
       " 133,\n",
       " 134,\n",
       " 135,\n",
       " 136,\n",
       " 137,\n",
       " 138,\n",
       " 139,\n",
       " 140,\n",
       " 141,\n",
       " 142,\n",
       " 143,\n",
       " 144,\n",
       " 145,\n",
       " 146,\n",
       " 147,\n",
       " 148,\n",
       " 149,\n",
       " 150,\n",
       " 151,\n",
       " 152,\n",
       " 153,\n",
       " 154,\n",
       " 155,\n",
       " 156,\n",
       " 157,\n",
       " 158,\n",
       " 159,\n",
       " 160,\n",
       " 161,\n",
       " 162,\n",
       " 163,\n",
       " 164,\n",
       " 165,\n",
       " 166,\n",
       " 167,\n",
       " 168,\n",
       " 169,\n",
       " 170,\n",
       " 171,\n",
       " 172,\n",
       " 173,\n",
       " 174,\n",
       " 175,\n",
       " 176,\n",
       " 177,\n",
       " 178,\n",
       " 179,\n",
       " 182,\n",
       " 183,\n",
       " 184,\n",
       " 186,\n",
       " 187,\n",
       " 189,\n",
       " 190,\n",
       " 191,\n",
       " 192,\n",
       " 193,\n",
       " 194,\n",
       " 195,\n",
       " 196,\n",
       " 197,\n",
       " 198,\n",
       " 199,\n",
       " 200,\n",
       " 201,\n",
       " 202,\n",
       " 203,\n",
       " 204,\n",
       " 205,\n",
       " 206,\n",
       " 207,\n",
       " 208,\n",
       " 209,\n",
       " 210,\n",
       " 211,\n",
       " 212,\n",
       " 213,\n",
       " 215,\n",
       " 216,\n",
       " 217,\n",
       " 219,\n",
       " 220,\n",
       " 221,\n",
       " 222,\n",
       " 223,\n",
       " 224,\n",
       " 225,\n",
       " 226,\n",
       " 227,\n",
       " 228,\n",
       " 229,\n",
       " 230,\n",
       " 231,\n",
       " 232,\n",
       " 233,\n",
       " 234,\n",
       " 235,\n",
       " 236,\n",
       " 237,\n",
       " 238,\n",
       " 239,\n",
       " 240,\n",
       " 241,\n",
       " 242,\n",
       " 243,\n",
       " 244,\n",
       " 245,\n",
       " 246,\n",
       " 247,\n",
       " 248,\n",
       " 249,\n",
       " 250,\n",
       " 251,\n",
       " 252,\n",
       " 253,\n",
       " 254,\n",
       " 255,\n",
       " 256,\n",
       " 257,\n",
       " 259,\n",
       " 260,\n",
       " 261,\n",
       " 262,\n",
       " 263,\n",
       " 264,\n",
       " 265,\n",
       " 266,\n",
       " 268,\n",
       " 269,\n",
       " 270,\n",
       " 271,\n",
       " 272,\n",
       " 273,\n",
       " 274,\n",
       " 275,\n",
       " 276,\n",
       " 277,\n",
       " 278,\n",
       " 279,\n",
       " 280,\n",
       " 281,\n",
       " 282,\n",
       " 283,\n",
       " 284,\n",
       " 285,\n",
       " 286,\n",
       " 287,\n",
       " 288,\n",
       " 289,\n",
       " 290,\n",
       " 291,\n",
       " 292,\n",
       " 293,\n",
       " 294,\n",
       " 295,\n",
       " 296,\n",
       " 297,\n",
       " 298,\n",
       " 299,\n",
       " 300,\n",
       " 301,\n",
       " 302,\n",
       " 303,\n",
       " 304,\n",
       " 305,\n",
       " 306,\n",
       " 307,\n",
       " 308,\n",
       " 309,\n",
       " 310,\n",
       " 311,\n",
       " 312,\n",
       " 313,\n",
       " 314,\n",
       " 315,\n",
       " 316,\n",
       " 317,\n",
       " 318,\n",
       " 319,\n",
       " 320,\n",
       " 321,\n",
       " 322,\n",
       " 323,\n",
       " 324,\n",
       " 325,\n",
       " 327,\n",
       " 328,\n",
       " 330,\n",
       " 331,\n",
       " 332,\n",
       " 333,\n",
       " 334,\n",
       " 336,\n",
       " 337,\n",
       " 338,\n",
       " 339,\n",
       " 340,\n",
       " 341,\n",
       " 342,\n",
       " 343,\n",
       " 344,\n",
       " 345,\n",
       " 346,\n",
       " 347,\n",
       " 348,\n",
       " 349,\n",
       " 350,\n",
       " 351,\n",
       " 352,\n",
       " 353,\n",
       " 354,\n",
       " 355,\n",
       " 356,\n",
       " 357,\n",
       " 358,\n",
       " 359,\n",
       " 360,\n",
       " 361,\n",
       " 362,\n",
       " 363,\n",
       " 364,\n",
       " 365,\n",
       " 366,\n",
       " 367,\n",
       " 368,\n",
       " 369,\n",
       " 370,\n",
       " 371,\n",
       " 372,\n",
       " 373,\n",
       " 374,\n",
       " 375,\n",
       " 376,\n",
       " 377,\n",
       " 378,\n",
       " 379,\n",
       " 380,\n",
       " 381,\n",
       " 383,\n",
       " 384,\n",
       " 385,\n",
       " 386,\n",
       " 387,\n",
       " 388,\n",
       " 389,\n",
       " 390,\n",
       " 391,\n",
       " 392,\n",
       " 393,\n",
       " 394,\n",
       " 395,\n",
       " 396,\n",
       " 397,\n",
       " 398,\n",
       " 399,\n",
       " 401,\n",
       " 402,\n",
       " 403,\n",
       " 404,\n",
       " 405,\n",
       " 406,\n",
       " 407,\n",
       " 408,\n",
       " 409,\n",
       " 410,\n",
       " 411,\n",
       " 412,\n",
       " 413,\n",
       " 414,\n",
       " 415,\n",
       " 417,\n",
       " 420,\n",
       " 421,\n",
       " 422,\n",
       " 423,\n",
       " 424,\n",
       " 425,\n",
       " 426,\n",
       " 427,\n",
       " 428,\n",
       " 429,\n",
       " 430,\n",
       " 431,\n",
       " 432,\n",
       " 433,\n",
       " 434,\n",
       " 435,\n",
       " 436,\n",
       " 437,\n",
       " 439,\n",
       " 440,\n",
       " 441,\n",
       " 442,\n",
       " 443,\n",
       " 444,\n",
       " 445,\n",
       " 446,\n",
       " 447,\n",
       " 448,\n",
       " 450,\n",
       " 451,\n",
       " 452,\n",
       " 454,\n",
       " 455,\n",
       " 457,\n",
       " 458,\n",
       " 459,\n",
       " 460,\n",
       " 461,\n",
       " 462,\n",
       " 463,\n",
       " 464,\n",
       " 465,\n",
       " 466,\n",
       " 467,\n",
       " 468,\n",
       " 469,\n",
       " 470,\n",
       " 471,\n",
       " 472,\n",
       " 473,\n",
       " 474,\n",
       " 475,\n",
       " 476,\n",
       " 477,\n",
       " 478,\n",
       " 479,\n",
       " 480,\n",
       " 481,\n",
       " 0,\n",
       " 8,\n",
       " 50,\n",
       " 51,\n",
       " 121,\n",
       " 127,\n",
       " 214,\n",
       " 267,\n",
       " 382,\n",
       " 418,\n",
       " 419,\n",
       " 438,\n",
       " 58,\n",
       " 84,\n",
       " 90,\n",
       " 110,\n",
       " 185,\n",
       " 326,\n",
       " 449,\n",
       " 456,\n",
       " 23,\n",
       " 125,\n",
       " 258,\n",
       " 400,\n",
       " 180,\n",
       " 181,\n",
       " 129,\n",
       " 112,\n",
       " 416,\n",
       " 67,\n",
       " 335,\n",
       " 453,\n",
       " 329,\n",
       " 78,\n",
       " 218,\n",
       " 188]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Tickers'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "117ed26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n4/35k7kmsn57n5yqhz89vtwcfr0000gn/T/ipykernel_61239/800084914.py:37: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if indiv_df['Ticker_Name'].values == \"AAL\":\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.5 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 55\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(y)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Split the data into training and testing sets\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    209\u001b[0m         )\n\u001b[1;32m    210\u001b[0m     ):\n\u001b[0;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    221\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/sklearn/model_selection/_split.py:2649\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2646\u001b[0m arrays \u001b[38;5;241m=\u001b[39m indexable(\u001b[38;5;241m*\u001b[39marrays)\n\u001b[1;32m   2648\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m-> 2649\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_shuffle_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2650\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_test_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.25\u001b[39;49m\n\u001b[1;32m   2651\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m   2654\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stratify \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/sklearn/model_selection/_split.py:2305\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2302\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(n_train), \u001b[38;5;28mint\u001b[39m(n_test)\n\u001b[1;32m   2304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_train \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWith n_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, test_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and train_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2308\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maforementioned parameters.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[1;32m   2309\u001b[0m     )\n\u001b[1;32m   2311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[0;31mValueError\u001b[0m: With n_samples=0, test_size=0.5 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "# # Initialize the LabelEncoder\n",
    "# label_encoder = LabelEncoder()\n",
    "\n",
    "# # Apply label encoding to the 'symbol' column\n",
    "# df['Tickers'] = label_encoder.fit_transform(df['Tickers'])\n",
    "\n",
    "# # Drop the original 'symbol' column if you no longer need it\n",
    "# df.drop(columns=['symbol'], inplace=True)\n",
    "\n",
    "# Define the number of time steps (lookback) and the number of features\n",
    "lookback = 10  # You can adjust this based on your dataset\n",
    "n_features = 10  # Update this to match the number of columns in your DataFrame\n",
    "\n",
    "# # Create sequences of data for training\n",
    "# def create_sequences(data, lookback):\n",
    "#     X, y = [], []\n",
    "#     for i in range(len(data) - lookback):\n",
    "#         # Convert the DataFrame to a NumPy array using .values\n",
    "#         X.append(data.iloc[i:i+lookback].values)  # Exclude 'date' and 'stock_price' columns\n",
    "#         y.append(data.iloc[i+lookback])\n",
    "#     return np.array(X), np.array(y)\n",
    "\n",
    "tickers = df['Tickers'].unique().tolist()\n",
    "list_of_df = []\n",
    "\n",
    "for ticker in tickers:\n",
    "    indiv_df = df[df['Tickers'] == ticker].copy()\n",
    "    indiv_df['Ticker_Name'] = ticker\n",
    "    list_of_df.append(indiv_df)\n",
    "    \n",
    "# Normalize the data using Min-Max scaling\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = []\n",
    "\n",
    "# for df in list_of_df:\n",
    "for indiv_df in list_of_df:\n",
    "    if indiv_df['Ticker_Name'].values == \"AAL\":\n",
    "        # Exclude 'date' & 'stock_price' columns\n",
    "        numeric_columns = indiv_df.columns.difference(['Date', 'stock_price'])\n",
    "        # Apply Min-Max scaling to the selected numeric columns\n",
    "        indiv_df[numeric_columns] = scaler.fit_transform(indiv_df[numeric_columns])\n",
    "        # Append the scaled DataFrame to scaled_data\n",
    "        scaled_data.append(indiv_df)\n",
    "# print(indiv_df)\n",
    "# # Create sequences of data\n",
    "# # X, y = create_sequences(scaled_data, lookback)\n",
    "\n",
    "# Assuming that your target variable is 'stock_price'\n",
    "X = [indiv_df.drop(columns=['Date', 'stock_price']).values for indiv_df in scaled_data]\n",
    "y = [indiv_df['stock_price'].values for indiv_df in scaled_data]\n",
    "print(X)\n",
    "print(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)\n",
    "\n",
    "# from sklearn.model_selection import LeaveOneOut\n",
    "# k = 3  # Number of folds (you can adjust this)\n",
    "# kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "# # Initialize a list to store the MSE for each fold\n",
    "# mse_scores = []\n",
    "\n",
    "# for train_idx, test_idx in kf.split(X):\n",
    "#     X_train, X_test = X[train_idx], X[test_idx]\n",
    "#     y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "#     # Train your model on X_train and y_train\n",
    "#     input_layer = Input(shape=(lookback, n_features), name='input_features')\n",
    "#     lstm_layer = LSTM(units=50, return_sequences=True)(input_layer)\n",
    "#     dropout_layer = Dropout(0.2)(lstm_layer)\n",
    "#     lstm_layer2 = LSTM(units=50, return_sequences=True)(dropout_layer)\n",
    "#     dropout_layer2 = Dropout(0.2)(lstm_layer2)\n",
    "#     lstm_layer3 = LSTM(units=50)(dropout_layer2)\n",
    "#     dropout_layer3 = Dropout(0.2)(lstm_layer3)\n",
    "#     output_layer = Dense(units=1)(dropout_layer3)\n",
    "\n",
    "#     model = Model(inputs=input_layer, outputs=output_layer)\n",
    "#     # Make predictions on the test set\n",
    "#     y_pred = model.predict(X_test)  # Replace 'model' with your model\n",
    "    \n",
    "#     # Store the predictions and ground truth\n",
    "#     mse = mean_squared_error(y_test, y_pred)\n",
    "#     mse_scores.append(mse)\n",
    "\n",
    "# # Calculate the mean squared error (MSE) across all iterations\n",
    "# average_mse = np.mean(mse_scores)\n",
    "# print(\"Mean Squared Error:\", average_mse)\n",
    "\n",
    "# print(X_train.shape)\n",
    "# print(X_test.shape)\n",
    "# # X_train = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
    "# # y_train = tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
    "# # X_test = tf.convert_to_tensor(X_test, dtype=tf.float32)\n",
    "# # y_test = tf.convert_to_tensor(y_test, dtype=tf.float32)\n",
    "# # (X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Build the LSTM model with time-aware features\n",
    "# model = Sequential()\n",
    "# model.add(layers.LSTM(units=10, activation='tanh', input_shape=(10,15)))\n",
    "# model.add(layers.Dense(1, activation=\"linear\"))\n",
    "\n",
    "# input_layer = Input(shape=(lookback, n_features), name='input_features')\n",
    "# lstm_layer = LSTM(units=50, return_sequences=True)(input_layer)\n",
    "# dropout_layer = Dropout(0.2)(lstm_layer)\n",
    "# lstm_layer2 = LSTM(units=50, return_sequences=True)(dropout_layer)\n",
    "# dropout_layer2 = Dropout(0.2)(lstm_layer2)\n",
    "# lstm_layer3 = LSTM(units=50)(dropout_layer2)\n",
    "# dropout_layer3 = Dropout(0.2)(lstm_layer3)\n",
    "# output_layer = Dense(units=1)(dropout_layer3)\n",
    "\n",
    "# model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# # Alternative: Build the LSTM model\n",
    "# # model = Sequential()\n",
    "# # model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], len(features))))\n",
    "# # model.add(LSTM(units=50, return_sequences=False))\n",
    "# # model.add(Dense(units=25))\n",
    "# # model.add(Dense(units=1))\n",
    "# #----#\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# # Train the model\n",
    "# model.fit(X_train, y_train, epochs=50, batch_size=10, verbose=1)\n",
    "\n",
    "# # Evaluate the model\n",
    "# train_loss = model.evaluate(X_train, y_train, verbose=0)\n",
    "# test_loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# print(f'Training Loss: {train_loss}')\n",
    "# print(f'Test Loss: {test_loss}')\n",
    "\n",
    "# # Make predictions\n",
    "# predictions = model.predict(X_test)\n",
    "\n",
    "# # Inverse transform the predictions to get the actual stock prices\n",
    "# predictions = scaler.inverse_transform(np.concatenate((X_test[:, -1, :-1], predictions), axis=1))[:, -1]\n",
    "\n",
    "# # Print or use predictions as needed\n",
    "# train_predictions = model.predict(X_train)\n",
    "# test_predictions = model.predict(X_test)\n",
    "\n",
    "# # Inverse transform the predictions to the original scale\n",
    "# train_predictions = scaler.inverse_transform(train_predictions)\n",
    "# test_predictions = scaler.inverse_transform(test_predictions)\n",
    "\n",
    "# # Plot the predictions\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.plot(data.index[sequence_length:len(X_train) + sequence_length], y_train, label='Actual Train Prices', color='blue')\n",
    "# plt.plot(data.index[len(X_train) + sequence_length:], y_test, label='Actual Test Prices', color='green')\n",
    "# plt.plot(data.index[sequence_length:len(X_train) + sequence_length], train_predictions, label='Predicted Train Prices', color='red')\n",
    "# plt.plot(data.index[len(X_train) + sequence_length:], test_predictions, label='Predicted Test Prices', color='orange')\n",
    "# plt.legend()\n",
    "# plt.xlabel('Date')\n",
    "# plt.ylabel('Stock Price')\n",
    "# plt.title('Stock Price Prediction')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5d9159",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
